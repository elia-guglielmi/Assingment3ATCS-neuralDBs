{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eaced97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"D:\\uni\\AdvancedTopics\\Assignment3\\DB\\card_games\\cards.csv\")\n",
    "df=df.sample(20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37151dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rule=pd.read_csv(r\"D:\\uni\\AdvancedTopics\\Assignment3\\DB\\card_games\\ruling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9496d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df_rule[df_rule['uuid'].isin(df['uuid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b7c17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"DB/card_games/cards_sample20.csv\")\n",
    "result.to_csv(\"DB/card_games/ruling_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca859c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Dict, Union, Optional,Any\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')\n",
    "model_name=os.getenv('MODEL_NAME')\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=groq_api_key)\n",
    "\n",
    "# ====================== GENERIC UDFs ======================\n",
    "\n",
    "def analyze_sentiment(text: str) -> float:\n",
    "    \"\"\"Sentiment Analysis: Returns score between -1 (negative) and 1 (positive)\"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Analyze sentiment of this text. Return ONLY a float between -1 (negative) and 1 (positive).\"\n",
    "        }, {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text\n",
    "        }],\n",
    "        model=model_name, \n",
    "        temperature=0.0,\n",
    "        max_tokens=10\n",
    "    )\n",
    "    return float(chat_completion.choices[0].message.content)\n",
    "\n",
    "def align_to_schema(data: Dict, domain: str) -> List[Dict]:\n",
    "    prompt = f\"\"\"Convert this table column mapping to Schema.org/{domain} property names:\n",
    "    For each column name in the input data, provide the most appropriate Schema.org property name.\n",
    "    The input data is: {data}\n",
    "    \n",
    "    Return ONLY a valid JSON array where each element is an object with:\n",
    "    - the original column name as key\n",
    "    - the corresponding Schema.org property name as value\n",
    "    \n",
    "    Example output format:\n",
    "    {{\n",
    "      \"name\": \"name\",\n",
    "      \"flavorText\": \"description\",\n",
    "      \"text\": \"text\",\n",
    "      \"type\": \"additionalType\"\n",
    "    }}\"\"\"\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt + \"\\n\\nJSON output required:\"\n",
    "        }],\n",
    "        model=model_name,\n",
    "        temperature=0.0,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    # Get the response and convert it to a list of mappings\n",
    "    result = json.loads(chat_completion.choices[0].message.content)\n",
    "    return result\n",
    "\n",
    "def classify_entity(text: str, classes: List[str]) -> str:\n",
    "    \"\"\"Entity Classification: Categorizes text into predefined classes\"\"\"\n",
    "    prompt = f\"\"\"Classify this text into one of {classes}:\n",
    "    {text}\n",
    "    Return ONLY the class name.\"\"\"\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=model_name,\n",
    "        temperature=0.0,\n",
    "        max_tokens=20\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "def summarize(text: str, max_length: int = 50) -> str:\n",
    "    \"\"\"Summarization: Condenses text to specified length\"\"\"\n",
    "    prompt = f\"\"\"Summarize this in under {max_length} characters:\n",
    "    {text}\n",
    "    Summary:\"\"\"\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=model_name,\n",
    "        temperature=0.0,\n",
    "        max_tokens=max_length\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "def extract_entities(text: str, entity_types: List[str]) -> List[str]:\n",
    "    prompt = f\"\"\"Extract ALL {entity_types} from this text:\n",
    "    \"{text}\"\n",
    "    Return ONLY a Python-style list like [\"item1\", \"item2\"] with no other text.\"\"\"\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=model_name,\n",
    "        temperature=0.0,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        return eval(chat_completion.choices[0].message.content)\n",
    "    except:\n",
    "        return []\n",
    "    try:\n",
    "        # Handle both direct string response and JSON object\n",
    "        response = chat_completion.choices[0].message.content\n",
    "        if isinstance(response, str):\n",
    "            return eval(response)\n",
    "        elif isinstance(response, dict):\n",
    "            return list(response.values())[0]  # Assumes first value is the list\n",
    "    except:\n",
    "        return []  # Fallback to empty list on parsing failure\n",
    "\n",
    "def impute_missing(\n",
    "    row: Dict[str, Any],\n",
    "    column_to_impute: str,\n",
    "    context_columns: List[str],\n",
    "    type_hint: Optional[str] = None,\n",
    "    id_columns: Optional[List[str]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generic missing value imputation using specified context columns.\n",
    "    \n",
    "    Args:\n",
    "        row: Input data row as dictionary\n",
    "        column_to_impute: Key containing missing value\n",
    "        context_columns: List of columns to use for context\n",
    "        type_hint: Expected data type (\"numeric\", \"date\", \"boolean\", \"text\")\n",
    "        id_columns: Columns to exclude from context (even if in context_columns)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with imputed value (or original if imputation fails)\n",
    "    \"\"\"\n",
    "    result_row=row.copy()\n",
    "    # Return early if no imputation needed\n",
    "    if row.get(column_to_impute) not in [None, \"NULL\", \"\", \"null\"] and not pd.isna(row.get(column_to_impute)):\n",
    "        return row[column_to_impute]\n",
    "    \n",
    "    # Filter context columns\n",
    "    id_columns = id_columns or []\n",
    "    available_context = [\n",
    "        (k, str(row[k])) for k in context_columns \n",
    "        if k in row and k != column_to_impute \n",
    "        and row[k] is not None and k not in id_columns\n",
    "    ]\n",
    "    \n",
    "    # Build context string\n",
    "    context_str = \"\\n\".join(f\"{k}: {v}\" for k, v in available_context)\n",
    "    if not context_str:\n",
    "        return row[column_to_impute]  # No context available\n",
    "    \n",
    "    # Determine type hint\n",
    "    type_instruction = \"\"\n",
    "    if type_hint:\n",
    "        type_map = {\n",
    "            \"numeric\": \"Return only a number without symbols\",\n",
    "            \"date\": \"Use YYYY-MM-DD format\",\n",
    "            \"boolean\": \"Return 'true' or 'false'\",\n",
    "            \"text\": \"Return a string without quotes\"\n",
    "        }\n",
    "        type_instruction = f\"\\nIMPORTANT: {type_map.get(type_hint, '')}\"\n",
    "    \n",
    "    prompt = f\"\"\"Impute missing value for '{column_to_impute}' based on this context:\n",
    "    {context_str}\n",
    "    {type_instruction}\n",
    "    Return ONLY the raw value with no additional text or formatting.\"\"\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=model_name,\n",
    "        temperature=0.0,\n",
    "        max_tokens=50\n",
    "    )\n",
    "    \n",
    "    result_row[column_to_impute] = chat_completion.choices[0].message.content.strip()\n",
    "    return result_row\n",
    "\n",
    "\n",
    "def augment_schema(row: Dict, new_field: str, prompt: str) -> Dict:\n",
    "    \"\"\"Schema Augmentation: Adds new derived field\"\"\"\n",
    "    user_prompt = f\"\"\"{prompt}\n",
    "    Data: {row}\n",
    "    Return ONLY the value for {new_field}.\"\"\"\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "        model=model_name,\n",
    "        temperature=0.0,\n",
    "        max_tokens=50\n",
    "    )\n",
    "    row[new_field] = chat_completion.choices[0].message.content.strip()\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b398e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "cards_df = pd.read_csv(\"DB/card_games/cards_sample20.csv\")\n",
    "rulings_df = pd.read_csv(\"DB/card_games/ruling_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26389a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ostiary Thrull']\n"
     ]
    }
   ],
   "source": [
    "def test_sentiment_analysis_001(cards: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Test Case ID: entity_classification_001\n",
    "    Calculates the sentiment score on cards' flavourtext and returns the negative ones\n",
    "    \"Which cards have flavor text with negative sentiment?\"\n",
    "    equivalent SQL:\n",
    "    SELECT name\n",
    "    FROM cards\n",
    "    WHERE flavorText IS NOT NULL\n",
    "    AND analyze_sentiment(flavorText) < 0;\n",
    "    \"\"\"\n",
    "    negative_cards = []\n",
    "    for _, card in cards.iterrows():\n",
    "        if pd.notna(card['flavorText']):\n",
    "            sentiment = analyze_sentiment(card['flavorText'])\n",
    "            if sentiment < 0:  # Negative sentiment\n",
    "                negative_cards.append(card['name'])\n",
    "    return negative_cards\n",
    "\n",
    "#print(test_sentiment_analysis_001(cards_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e6b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control\n"
     ]
    }
   ],
   "source": [
    "def test_entity_classification_001(cards: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Test Case ID: entity_classification_001\n",
    "    Classifies the archetype of 'Twilight Prophet' based on text using\n",
    "    \"What is the archetype of Twilight Prophet?\"\n",
    "    equivalent SQL:\n",
    "    SELECT \n",
    "    'Twilight Prophet' AS card_name,\n",
    "    classify_entity(\n",
    "        text, \n",
    "        ARRAY['Aggro', 'Control', 'Combo']  -- Array of possible archetypes\n",
    "    ) AS archetype\n",
    "    FROM cards\n",
    "    WHERE name = 'Twilight Prophet'\n",
    "    LIMIT 1; \n",
    "    \"\"\"\n",
    "    # Get the specific card data\n",
    "    card = cards[cards['name'] == 'Twilight Prophet'].iloc[0]\n",
    "    archetype=classify_entity(card['text'],[\"Aggro\",\"Control\",\"Combo\"])\n",
    "    return archetype\n",
    "\n",
    "#print(test_entity_classification_001(cards_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3debd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Crested Sunmare', 'Civic Wayfinder', 'Helm of the Gods', 'Ancestral Memories', 'Lady Sun']\n"
     ]
    }
   ],
   "source": [
    "def test_entity_recognition_001(cards: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Test Case ID: entity_recognition_001\n",
    "    Identifies people or creature entities in a text and returns all the cards that present identified entities in their flavour text\n",
    "    Which cards mention creatures or people in their flavor text?\n",
    "    equivalent SQL:\n",
    "    SELECT DISTINCT name\n",
    "    FROM cards\n",
    "    WHERE flavorText IS NOT NULL\n",
    "    AND extract_entities(flavorText, ARRAY['PERSON','CREATURE']::text[]) IS NOT NULL;\n",
    "    \n",
    "    \"\"\"\n",
    "    entity_cards = []\n",
    "    for _, card in cards.iterrows():\n",
    "        if pd.notna(card['flavorText']):\n",
    "            entities = extract_entities(card['flavorText'], [\"PERSON\",\"CREATURE\"])\n",
    "            if entities:  # Found mentions of people/creatures\n",
    "                entity_cards.append(card['name'])\n",
    "    return entity_cards\n",
    "\n",
    "#print(test_entity_recognition_001(cards_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd052f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Lady Sun', 'mapped_properties': {'name': 'name', 'flavorText': 'description', 'text': 'gameplayDescription', 'type': 'gameItemVariant'}}]\n"
     ]
    }
   ],
   "source": [
    "def test_schema_alignment_001(cards: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Test Case ID: schema_alignment_001  \n",
    "    Map card properties to Schema.org types.\n",
    "    equivalent SQL:\n",
    "    SELECT \n",
    "        name,\n",
    "        align_to_schema(\n",
    "            JSON_BUILD_OBJECT(\n",
    "                'name', name,\n",
    "                'flavorText', flavorText,\n",
    "                'text', text,\n",
    "                'type', type\n",
    "            ),\n",
    "            'Game'\n",
    "        ) AS mapped_properties\n",
    "    FROM cards\n",
    "    WHERE name = 'Lady Sun';\n",
    "      \n",
    "    \"\"\"\n",
    "    aligned = []\n",
    "    for _, card in cards.iterrows():\n",
    "        if card['name'] == \"Lady Sun\":  # From test case\n",
    "            mapped = align_to_schema({\n",
    "                \"name\": card['name'],\n",
    "                \"flavorText\": card['flavorText'],\n",
    "                \"text\": card['text'],\n",
    "                \"type\": card['type']\n",
    "            }, \"Game\")\n",
    "            aligned.append({\n",
    "                \"name\": card['name'],\n",
    "                \"mapped_properties\": mapped\n",
    "            })\n",
    "    return aligned\n",
    "#print(test_schema_alignment_001(cards_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be693abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Esper Sojourners', 'imputed_toughness': '2'}, {'name': 'Dream Stalker', 'imputed_toughness': '2'}, {'name': 'Civic Wayfinder', 'imputed_toughness': '3'}]\n"
     ]
    }
   ],
   "source": [
    "def test_imputation_missing_001(cards: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Test Case ID: imputation_missing_001\n",
    "    Calls the llm to try and fill a specific column value based on external knowledge and the other columns values \n",
    "    \"Fill in the missing toughness value of creatures based on other card fields.\"\n",
    "    equivalent SQL:\n",
    "    WITH creature_cards AS (\n",
    "    SELECT *\n",
    "    FROM cards\n",
    "    WHERE LOWER(type) LIKE '%creature%'\n",
    "    ),\n",
    "    cards_to_impute AS (\n",
    "    SELECT *\n",
    "    FROM creature_cards\n",
    "    WHERE toughness IS NULL \n",
    "        OR toughness = '' \n",
    "        OR LOWER(toughness) = 'null'\n",
    "    )\n",
    "    SELECT \n",
    "    name,\n",
    "    impute_missing(\n",
    "        JSON_BUILD_OBJECT(\n",
    "        'name', name,\n",
    "        'type', type,\n",
    "        'power', power,\n",
    "        'text', text\n",
    "        ),\n",
    "        'toughness',\n",
    "        ARRAY['name', 'type', 'power', 'text'],\n",
    "        'numeric'\n",
    "    ) AS imputed_toughness\n",
    "    FROM cards_to_impute;\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get the creature type cards\n",
    "    creature_cards = cards_df[cards_df['type'].str.contains('Creature', case=False, na=False)].copy()\n",
    "    # List of cards to modify \n",
    "    cards_to_update = [\"Civic Wayfinder\", \"Dream Stalker\", \"Esper Sojourners\"]\n",
    "\n",
    "    # Update toughness to None/NULL for those specific cards\n",
    "    creature_cards.loc[creature_cards['name'].isin(cards_to_update), 'toughness'] = None\n",
    "    imputed_cards=[]\n",
    "    creature_cards=creature_cards.to_dict('records')\n",
    "    for card in creature_cards:\n",
    "        if pd.isna(card[\"toughness\"]) or card[\"toughness\"] in [None, \"NULL\", \"\", \"null\"]:\n",
    "            imputed_cards.append({\"name\":card[\"name\"],\"imputed_toughness\":impute_missing(\n",
    "        row=card,\n",
    "        column_to_impute=\"toughness\",\n",
    "        context_columns=[\"name\", \"type\", \"power\", \"text\"],\n",
    "        type_hint=\"numeric\")[\"toughness\"]})\n",
    "    return(imputed_cards)\n",
    "    \n",
    "print(test_imputation_missing_001(cards_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ab784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Esper Sojourners', 'summary': 'Cycling is an activated ability, interacts with ability effects, not spell effects.'}]\n"
     ]
    }
   ],
   "source": [
    "def test_summarization_001(cards: pd.DataFrame, rulings: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Test Case ID: summarization_001\n",
    "    Calls the llm to summarize a text\n",
    "    \"Summarize rulings for 'Esper Sojourners'.\"\n",
    "    equivalent SQL:\n",
    "    WITH card_uuid AS (\n",
    "    SELECT uuid\n",
    "    FROM cards\n",
    "    WHERE name = 'Esper Sojourners'\n",
    "    ),\n",
    "    combined_rulings AS (\n",
    "    SELECT \n",
    "        c.name,\n",
    "        STRING_AGG(r.text, E'\\n' ORDER BY r.text) AS rulings_text\n",
    "    FROM card_uuid cu\n",
    "    JOIN cards c ON c.uuid = cu.uuid\n",
    "    JOIN rulings r ON r.uuid = cu.uuid\n",
    "    GROUP BY c.name\n",
    "    )\n",
    "    SELECT \n",
    "    name,\n",
    "    summarize(rulings_text, 100) AS summary\n",
    "    FROM combined_rulings;\n",
    "    \n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    card_name = \"Esper Sojourners\"\n",
    "    card_uuid = cards[cards['name'] == card_name]['uuid'].iloc[0]\n",
    "    card_rulings = rulings[rulings['uuid'] == card_uuid]\n",
    "    \n",
    "    if not card_rulings.empty:\n",
    "        combined_rulings = \"\\n\".join(card_rulings['text'])\n",
    "        summary = summarize(combined_rulings, 100)\n",
    "        summaries.append({\n",
    "            \"name\": card_name,\n",
    "            \"summary\": summary\n",
    "        })\n",
    "    return summaries\n",
    "\n",
    "#print(test_summarization_001(cards_df,rulings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce39f429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Twilight Prophet', 'Lady Sun', 'Esper Sojourners', 'Illusions of Grandeur', 'Helm of the Gods', 'Crested Sunmare', 'Blizzard Brawl']\n"
     ]
    }
   ],
   "source": [
    "def test_classification_002(cards: pd.DataFrame, rulings: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Test Case ID: classification_002\n",
    "    Identifies cards with rulings classified as \"damage_prevention\" \n",
    "    \"Which cards have rulings that mention 'damage prevention'?\"\n",
    "    equivalent SQL:\n",
    "    SELECT DISTINCT c.name\n",
    "    FROM rulings r\n",
    "    JOIN cards c ON r.uuid = c.uuid\n",
    "    WHERE classify_entity(r.text, ARRAY['damage_prevention', 'other']) = 'damage_prevention';\n",
    "    \n",
    "    \"\"\"\n",
    "    damage_prevention_cards = []\n",
    "    \n",
    "    for _, ruling in rulings.iterrows():\n",
    "        # Classify the ruling text\n",
    "        classification = classify_entity(\n",
    "            text=ruling['text'],\n",
    "            classes=[\"damage_prevention\", \"other\"],\n",
    "        )\n",
    "        \n",
    "        if classification == \"damage_prevention\":\n",
    "            card = cards[cards['uuid'] == ruling['uuid']].iloc[0]\n",
    "            damage_prevention_cards.append(card['name'])\n",
    "    \n",
    "    return list(set(damage_prevention_cards))\n",
    "#print(test_classification_002(cards_df,rulings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f6ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'Artifact Creature — Horse', 'buff_count': 1, 'total_cards': 1}, {'type': 'Artifact Creature — Vedalken Wizard', 'buff_count': 0, 'total_cards': 1}, {'type': 'Artifact — Equipment', 'buff_count': 1, 'total_cards': 1}, {'type': 'Creature — Elf Druid Warrior', 'buff_count': 0, 'total_cards': 1}, {'type': 'Creature — Horse', 'buff_count': 1, 'total_cards': 1}, {'type': 'Creature — Illusion', 'buff_count': 0, 'total_cards': 1}, {'type': 'Creature — Merfolk Soldier', 'buff_count': 0, 'total_cards': 1}, {'type': 'Creature — Thrull', 'buff_count': 0, 'total_cards': 1}, {'type': 'Creature — Vampire Cleric', 'buff_count': 0, 'total_cards': 1}, {'type': 'Enchantment', 'buff_count': 2, 'total_cards': 2}, {'type': 'Instant', 'buff_count': 1, 'total_cards': 2}, {'type': 'Land', 'buff_count': 1, 'total_cards': 3}, {'type': 'Legendary Creature — Human Advisor', 'buff_count': 0, 'total_cards': 1}, {'type': 'Snow Sorcery', 'buff_count': 1, 'total_cards': 1}, {'type': 'Sorcery', 'buff_count': 0, 'total_cards': 2}]\n"
     ]
    }
   ],
   "source": [
    "def test_entity_recognition_002(cards: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Test Case ID: entity_recognition_002\n",
    "    call llm to analyze the text to identify cards that give a buff and count how many identified cards for each type\n",
    "    How many cards of each type have text that inplies giving a buff?\n",
    "    equivalent SQL:\n",
    "    SELECT \n",
    "        type,\n",
    "        COUNT(CASE WHEN extract_entities(text, ARRAY['buff', 'stat increase']) IS NOT NULL THEN 1 END) AS buff_count,\n",
    "        COUNT(*) AS total_cards\n",
    "    FROM cards\n",
    "    WHERE text IS NOT NULL\n",
    "    GROUP BY type;\n",
    "    \n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Group by card type and process each group\n",
    "    for card_type, group in cards.groupby('type'):\n",
    "        buff_count = 0\n",
    "        \n",
    "        for text in group['text'].dropna():  # Skip NaN values\n",
    "            extracted = extract_entities(text, [\"buff\", \"stat increase\"])\n",
    "            if extracted:\n",
    "                buff_count += 1\n",
    "                \n",
    "        \n",
    "        results.append({\n",
    "            \"type\": card_type,\n",
    "            \"buff_count\": buff_count,\n",
    "            \"total_cards\": len(group)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "#print(test_group_by_001(cards_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083437fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.7\n",
      "0.0\n",
      "0.08\n",
      "0.75\n",
      "0.8\n",
      "0.0\n",
      "-0.5\n",
      "0.0\n",
      "0.0\n",
      "0.75\n",
      "[{'name': 'Crested Sunmare', 'summary': \"Crested Sunmare's ability triggers if you gained life earlier in the turn, not during the end step.\", 'sentiment': 'positive'}, {'name': 'Helm of the Gods', 'summary': \"You control Aura spells targeting opponents' permanents.\", 'sentiment': 'positive'}]\n"
     ]
    }
   ],
   "source": [
    "def test_analyze_sentiment_summarize_001(cards: pd.DataFrame, rulings: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Test Case ID: analyze_sentiment_summarize_001\n",
    "    calls an llm to assing a sentiment score to flavour text of the cards,\n",
    "    than takes the cards with positive score(>0.5)\n",
    "    and finds those wid rulings, finally calls again the llm to produce\n",
    "    a summary of said rulings\n",
    "    \"Cards with positive flavor text sentiment and complex rulings.\"\n",
    "    equivalent SQL:\n",
    "    SELECT\n",
    "    c.name,\n",
    "    summarize(string_agg(r.text, E'\\n'), 100) AS summary,\n",
    "    'positive' AS sentiment\n",
    "    FROM cards c\n",
    "    JOIN rulings r ON c.uuid = r.uuid\n",
    "    WHERE c.flavorText IS NOT NULL\n",
    "    AND analyze_sentiment(c.flavorText) > 0.5\n",
    "    GROUP BY c.name;\n",
    "    \n",
    "    \"\"\"\n",
    "    complex_cards = []\n",
    "    for _, card in cards.iterrows():\n",
    "        if pd.notna(card['flavorText']):\n",
    "            sentiment = analyze_sentiment(card['flavorText'])\n",
    "            if sentiment > 0.5:\n",
    "                card_rulings = rulings[rulings['uuid'] == card['uuid']]\n",
    "                if not card_rulings.empty:\n",
    "                    combined_rulings = \"\\n\".join(card_rulings['text'])\n",
    "                    summary = summarize(combined_rulings, 100)\n",
    "                    complex_cards.append({\n",
    "                        \"name\": card['name'],\n",
    "                        \"summary\": summary,\n",
    "                        \"sentiment\": \"positive\"\n",
    "                    })\n",
    "    return complex_cards\n",
    "#print(test_analyze_sentiment_summarize_001(cards_df,rulings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29979584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Crested Sunmare', 'imputed_rarity': 'Rare', 'archetype': 'midrange'}, {'name': 'Dream Stalker', 'imputed_rarity': 'Rare', 'archetype': 'control'}, {'name': 'Ostiary Thrull', 'imputed_rarity': 'Uncommon', 'archetype': 'control'}]\n"
     ]
    }
   ],
   "source": [
    "def test_entity_impute_classify_001(cards: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Test Case ID: analyze_sentiment_summarize_001\n",
    "    For creature cards with missing rarity:\n",
    "    finds the cards with missing rarity  values, calls the llm to predict it and\n",
    "    classify archetype\n",
    "    \"For creature cards with missing rarity, predict the rarity based on their stats and classify them by their gameplay archetype.\"\n",
    "    equivalent SQL:\n",
    "    WITH missing_rarity_creatures AS (\n",
    "    SELECT *\n",
    "    FROM cards\n",
    "    WHERE LOWER(type) LIKE '%creature%'\n",
    "        AND rarity IS NULL\n",
    "    ),\n",
    "    processed_cards AS (\n",
    "    SELECT\n",
    "        name,\n",
    "        impute_missing(\n",
    "        JSON_BUILD_OBJECT(\n",
    "            'name', name,\n",
    "            'type', type,\n",
    "            'power', power,\n",
    "            'toughness', toughness,\n",
    "            'text', text,\n",
    "            'manaCost', manaCost\n",
    "        ),\n",
    "        'rarity',\n",
    "        ARRAY['name', 'type', 'power', 'toughness', 'text', 'manaCost'],\n",
    "        'text'\n",
    "        )->>'rarity' AS imputed_rarity,\n",
    "        classify_entity(\n",
    "        CONCAT(\n",
    "            'Card: ', name, '\\n',\n",
    "            'Stats: ', power, '/', toughness, ' (MV: ', manaCost, ')\\n',\n",
    "            'Text: ', COALESCE(text, '')\n",
    "        ),\n",
    "        ARRAY['aggro', 'control', 'combo', 'midrange', 'hybrid']\n",
    "        ) AS archetype\n",
    "    FROM missing_rarity_creatures\n",
    "    )\n",
    "    SELECT\n",
    "    name,\n",
    "    imputed_rarity,\n",
    "    LOWER(archetype) AS archetype\n",
    "    FROM processed_cards;\n",
    "    \n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Force rarity=None for our test cases\n",
    "    cards.loc[cards['name'].isin([\n",
    "        \"Dream Stalker\", \n",
    "        \"Ostiary Thrull\", \n",
    "        \"Crested Sunmare\"\n",
    "    ]), 'rarity'] = None\n",
    "    \n",
    "    # Filter creatures with missing rarity\n",
    "    creatures = cards[\n",
    "        (cards['type'].str.contains('Creature', case=False, na=False)) & \n",
    "        (cards['rarity'].isna())\n",
    "    ].copy()\n",
    "    \n",
    "    for _, card in creatures.iterrows():\n",
    "        # Convert card data to dict for UDF processing\n",
    "        card_data = card.to_dict()\n",
    "        \n",
    "        # Impute missing rarity\n",
    "        imputed_card = impute_missing(\n",
    "            row=card_data,\n",
    "            column_to_impute=\"rarity\",\n",
    "            context_columns=[\"name\", \"type\", \"power\", \"toughness\", \"text\", \"manaCost\"],\n",
    "            type_hint=\"text\"\n",
    "        )\n",
    "        \n",
    "        #Classify archetype\n",
    "        archetype = classify_entity(\n",
    "            text=f\"\"\"\n",
    "            Card: {card['name']}\n",
    "            Stats: {card['power']}/{card['toughness']} (MV: {card['manaCost']})\n",
    "            Text: {card['text']}\n",
    "            \"\"\",\n",
    "            classes=[\"aggro\", \"control\", \"combo\", \"midrange\", \"hybrid\"],\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"name\": card['name'],\n",
    "            \"imputed_rarity\": imputed_card[\"rarity\"],\n",
    "            \"archetype\": archetype.lower()\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "#print(test_entity_impute_classify_001(cards_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIRD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
