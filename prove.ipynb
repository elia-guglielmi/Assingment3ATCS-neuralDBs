{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eaced97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb07d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling dataset creation\n",
    "df=pd.read_csv(r\"D:\\uni\\AdvancedTopics\\Assignment3\\DB\\card_games\\cards.csv\")\n",
    "df=df.sample(20,random_state=42)\n",
    "df_rule=pd.read_csv(r\"D:\\uni\\AdvancedTopics\\Assignment3\\DB\\card_games\\ruling.csv\")\n",
    "result = df_rule[df_rule['uuid'].isin(df['uuid'])]\n",
    "users=pd.read_csv(r\"DB\\codebase\\users.csv\")\n",
    "users=users.sample(20,random_state=42)\n",
    "#users.to_csv(\"DB/codebase/users20sample.csv\")\n",
    "posts=pd.read_csv(r\"DB\\codebase\\posts.csv\")\n",
    "posts=posts.sample(20,random_state=42)\n",
    "#posts.to_csv(\"DB/codebase/posts20sample.csv\")\n",
    "#df.to_csv(\"DB/card_games/cards_sample20.csv\")\n",
    "#result.to_csv(\"DB/card_games/ruling_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a05aa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Dict, Union, Optional,Any\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')\n",
    "model_name=os.getenv('MODEL_NAME')\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=groq_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca859c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================== GENERIC UDFs zero shot ======================\n",
    "\n",
    "def analyze_sentiment(text: str) -> float:\n",
    "    \"\"\"Sentiment Analysis: Returns score between -1 (negative) and 1 (positive)\"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Analyze sentiment of this text. Return ONLY a float between -1 (negative) and 1 (positive).\"\n",
    "        }, {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text\n",
    "        }],\n",
    "        model=model_name, \n",
    "        temperature=0.0,\n",
    "        max_tokens=10\n",
    "    )\n",
    "    return float(chat_completion.choices[0].message.content)\n",
    "\n",
    "def align_to_schema(data: Dict, domain: str) -> List[Dict]:\n",
    "    prompt = f\"\"\"Convert this table column mapping to Schema.org/{domain} property names:\n",
    "    For each column name in the input data, provide the most appropriate Schema.org property name.\n",
    "    The input data is: {data}\n",
    "    \n",
    "    Return ONLY a valid JSON array where each element is an object with:\n",
    "    - the original column name as key\n",
    "    - the corresponding Schema.org property name as value\n",
    "    \n",
    "    Example output format:\n",
    "    {{\n",
    "      \"name\": \"name\",\n",
    "      \"flavorText\": \"description\",\n",
    "      \"text\": \"text\",\n",
    "      \"type\": \"additionalType\"\n",
    "    }}\"\"\"\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt + \"\\n\\nJSON output required:\"\n",
    "        }],\n",
    "        model=model_name,\n",
    "        temperature=0.0,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    # Get the response and convert it to a list of mappings\n",
    "    result = json.loads(chat_completion.choices[0].message.content)\n",
    "    return result\n",
    "\n",
    "def classify_entity(text: str, classes: List[str]) -> str:\n",
    "    \"\"\"Entity Classification: Categorizes text into predefined classes\"\"\"\n",
    "    prompt = f\"\"\"Classify this text into one of {classes}:\n",
    "    {text}\n",
    "    Return ONLY the class name.\"\"\"\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=model_name,\n",
    "        temperature=0.0,\n",
    "        max_tokens=20\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "def summarize(text: str, max_length: int = 50) -> str:\n",
    "    \"\"\"Summarization: Condenses text to specified length\"\"\"\n",
    "    prompt = f\"\"\"Summarize this in under {max_length} characters:\n",
    "    {text}\n",
    "    Summary:\"\"\"\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=model_name,\n",
    "        temperature=0.0,\n",
    "        max_tokens=max_length\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "def extract_entities(text: str, entity_types: List[str]) -> List[str]:\n",
    "    prompt = f\"\"\"Extract ALL {entity_types} from this text:\n",
    "    \"{text}\"\n",
    "    Return ONLY a Python-style list like [\"item1\", \"item2\"] with no other text.\"\"\"\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=model_name,\n",
    "        temperature=0.0,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        return eval(chat_completion.choices[0].message.content)\n",
    "    except:\n",
    "        return []\n",
    "    try:\n",
    "        # Handle both direct string response and JSON object\n",
    "        response = chat_completion.choices[0].message.content\n",
    "        if isinstance(response, str):\n",
    "            return eval(response)\n",
    "        elif isinstance(response, dict):\n",
    "            return list(response.values())[0]  # Assumes first value is the list\n",
    "    except:\n",
    "        return []  # Fallback to empty list on parsing failure\n",
    "\n",
    "def impute_missing(\n",
    "    row: Dict[str, Any],\n",
    "    column_to_impute: str,\n",
    "    context_columns: List[str],\n",
    "    type_hint: Optional[str] = None,\n",
    "    id_columns: Optional[List[str]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generic missing value imputation using specified context columns.\n",
    "    \n",
    "    Args:\n",
    "        row: Input data row as dictionary\n",
    "        column_to_impute: Key containing missing value\n",
    "        context_columns: List of columns to use for context\n",
    "        type_hint: Expected data type (\"numeric\", \"date\", \"boolean\", \"text\")\n",
    "        id_columns: Columns to exclude from context (even if in context_columns)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with imputed value (or original if imputation fails)\n",
    "    \"\"\"\n",
    "    result_row=row.copy()\n",
    "    # Return early if no imputation needed\n",
    "    if row.get(column_to_impute) not in [None, \"NULL\", \"\", \"null\"] and not pd.isna(row.get(column_to_impute)):\n",
    "        return row[column_to_impute]\n",
    "    \n",
    "    # Filter context columns\n",
    "    id_columns = id_columns or []\n",
    "    available_context = [\n",
    "        (k, str(row[k])) for k in context_columns \n",
    "        if k in row and k != column_to_impute \n",
    "        and row[k] is not None and k not in id_columns\n",
    "    ]\n",
    "    \n",
    "    # Build context string\n",
    "    context_str = \"\\n\".join(f\"{k}: {v}\" for k, v in available_context)\n",
    "    if not context_str:\n",
    "        return row[column_to_impute]  # No context available\n",
    "    \n",
    "    # Determine type hint\n",
    "    type_instruction = \"\"\n",
    "    if type_hint:\n",
    "        type_map = {\n",
    "            \"numeric\": \"Return only a number without symbols\",\n",
    "            \"date\": \"Use YYYY-MM-DD format\",\n",
    "            \"boolean\": \"Return 'true' or 'false'\",\n",
    "            \"text\": \"Return a string without quotes\"\n",
    "        }\n",
    "        type_instruction = f\"\\nIMPORTANT: {type_map.get(type_hint, '')}\"\n",
    "    \n",
    "    prompt = f\"\"\"Impute missing value for '{column_to_impute}' based on this context:\n",
    "    {context_str}\n",
    "    {type_instruction}\n",
    "    Return ONLY the raw value with no additional text or formatting.\"\"\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=model_name,\n",
    "        temperature=0.0,\n",
    "        max_tokens=50\n",
    "    )\n",
    "    \n",
    "    result_row[column_to_impute] = chat_completion.choices[0].message.content.strip()\n",
    "    return result_row\n",
    "\n",
    "\n",
    "def augment_schema(row: Dict, new_field: str, prompt: str) -> Dict:\n",
    "    \"\"\"Schema Augmentation: Adds new derived field\"\"\"\n",
    "    user_prompt = f\"\"\"{prompt}\n",
    "    Data: {row}\n",
    "    Return ONLY the value for {new_field}.\"\"\"\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "        model=model_name,\n",
    "        temperature=0.0,\n",
    "        max_tokens=50\n",
    "    )\n",
    "    row[new_field] = chat_completion.choices[0].message.content.strip()\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4140c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================== GENERIC UDFs few shot ======================\n",
    "def analyze_sentiment(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Sentiment Analysis with few-shot learning: \n",
    "    Returns score between -1 (negative) and 1 (positive)\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    \n",
    "    # Few-shot examples to improve sentiment analysis accuracy\n",
    "    few_shot_examples = [\n",
    "        {\"text\": \"I absolutely love this product!\", \"sentiment\": 0.9},\n",
    "        {\"text\": \"This is the worst experience ever.\", \"sentiment\": -0.8},\n",
    "        {\"text\": \"It's okay, nothing special.\", \"sentiment\": 0.1},\n",
    "        {\"text\": \"Incredibly disappointed and frustrated.\", \"sentiment\": -0.7},\n",
    "        {\"text\": \"Amazing service and great quality!\", \"sentiment\": 0.95}\n",
    "    ]\n",
    "    \n",
    "    # Construct messages with few-shot examples\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Analyze sentiment of text. Return ONLY a float between -1 (negative) and 1 (positive).\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Add few-shot examples\n",
    "    for example in few_shot_examples:\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": example[\"text\"]\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": str(example[\"sentiment\"])\n",
    "        })\n",
    "    \n",
    "    # Add current text to analyze\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": text\n",
    "    })\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model_name,  \n",
    "        temperature=0.0, \n",
    "        max_tokens=10\n",
    "    )\n",
    "    return float(chat_completion.choices[0].message.content)\n",
    "\n",
    "def align_to_schema(data: Dict, domain: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Schema alignment with few-shot examples\n",
    "    Converts table column mapping to Schema.org property names\n",
    "    \"\"\"\n",
    "    # Few-shot examples for different domains\n",
    "    few_shot_examples = {\n",
    "        \"Product\": [\n",
    "            {\n",
    "                \"input\": {\"name\": \"iPhone 12\", \"price\": 799, \"color\": \"Blue\"},\n",
    "                \"output\": {\n",
    "                    \"name\": \"name\",\n",
    "                    \"price\": \"offers.price\",\n",
    "                    \"color\": \"color\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"input\": {\"brand\": \"Apple\", \"description\": \"Smartphone\", \"weight\": \"164g\"},\n",
    "                \"output\": {\n",
    "                    \"brand\": \"brand\",\n",
    "                    \"description\": \"description\",\n",
    "                    \"weight\": \"weight\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Person\": [\n",
    "            {\n",
    "                \"input\": {\"first_name\": \"John\", \"birth_year\": 1990, \"job_title\": \"Engineer\"},\n",
    "                \"output\": {\n",
    "                    \"first_name\": \"givenName\",\n",
    "                    \"birth_year\": \"birthDate\",\n",
    "                    \"job_title\": \"jobTitle\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Select few-shot examples for the specific domain\n",
    "    domain_examples = few_shot_examples.get(domain, few_shot_examples[\"Product\"])\n",
    "    \n",
    "    # Construct messages with few-shot examples\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Convert table column mapping to Schema.org/{domain} property names. Return ONLY a valid JSON mapping.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Add few-shot examples\n",
    "    for example in domain_examples:\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Input data: {json.dumps(example['input'])}\"\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": json.dumps(example['output'])\n",
    "        })\n",
    "    \n",
    "    # Add current data to map\n",
    "    messages.append({\n",
    "        \"role\": \"user\", \n",
    "        \"content\": f\"Input data: {json.dumps(data)}\\n\\nJSON output required:\"\n",
    "    })\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model_name, \n",
    "        temperature=0.0, \n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    # Get the response and convert it to a list of mappings\n",
    "    result = json.loads(chat_completion.choices[0].message.content)\n",
    "    return result\n",
    "\n",
    "def classify_entity(text: str, classes: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Entity Classification with few-shot learning\n",
    "    Categorizes text into predefined classes\n",
    "    \"\"\"\n",
    "    # Few-shot examples for different classification scenarios\n",
    "    few_shot_examples = [\n",
    "        {\"text\": \"Breaking news about tech innovation\", \"class\": \"Technology\"},\n",
    "        {\"text\": \"Quarterly financial results announced\", \"class\": \"Finance\"},\n",
    "        {\"text\": \"New medical breakthrough in cancer research\", \"class\": \"Science\"},\n",
    "        {\"text\": \"Political debate heats up before election\", \"class\": \"Politics\"},\n",
    "        {\"text\": \"Concert tickets sold out in minutes\", \"class\": \"Entertainment\"}\n",
    "    ]\n",
    "    \n",
    "    # Construct messages with few-shot examples\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Classify text into one of {classes}. Return ONLY the class name.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Add few-shot examples\n",
    "    for example in few_shot_examples:\n",
    "        if example[\"class\"] in classes:\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example[\"text\"]\n",
    "            })\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": example[\"class\"]\n",
    "            })\n",
    "    \n",
    "    # Add current text to classify\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Classify this text into one of {classes}: {text}\"\n",
    "    })\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model_name, \n",
    "        temperature=0.0, \n",
    "        max_tokens=20\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "def summarize(text: str, max_length: int = 50) -> str:\n",
    "    \"\"\"\n",
    "    Summarization with few-shot learning\n",
    "    Condenses text to specified length\n",
    "    \"\"\"\n",
    "    # Few-shot examples of summarization\n",
    "    few_shot_examples = [\n",
    "        {\n",
    "            \"text\": \"The quick brown fox jumps over the lazy dog. It was a beautiful day in the forest with sunlight streaming through the trees.\",\n",
    "            \"summary\": \"Fox jumps over lazy dog on a beautiful day.\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"Machine learning is a complex field of artificial intelligence that involves training algorithms to learn from and make predictions or decisions based on data.\",\n",
    "            \"summary\": \"ML is AI that trains algorithms to learn from data.\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"Climate change is causing significant global environmental challenges, including rising sea levels, more frequent extreme weather events, and disruptions to ecosystems.\",\n",
    "            \"summary\": \"Climate change threatens environment with sea rise and extreme weather.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Construct messages with few-shot examples\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Summarize text in under {max_length} characters, capturing key points.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Add few-shot examples\n",
    "    for example in few_shot_examples:\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": example[\"text\"]\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": example[\"summary\"]\n",
    "        })\n",
    "    \n",
    "    # Add current text to summarize\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Summarize this in under {max_length} characters: {text}\"\n",
    "    })\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model_name, \n",
    "        temperature=0.0, \n",
    "        max_tokens=max_length\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "def extract_entities(text: str, entity_types: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Entity Extraction with few-shot learning\n",
    "    Extracts specified entity types from text\n",
    "    \"\"\"\n",
    "    # Few-shot examples of entity extraction\n",
    "    few_shot_examples = [\n",
    "        {\n",
    "            \"text\": \"Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976.\",\n",
    "            \"entities\": {\n",
    "                \"organizations\": [\"Apple Inc.\"],\n",
    "                \"persons\": [\"Steve Jobs\"],\n",
    "                \"locations\": [\"Cupertino\", \"California\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"The World Health Organization reported a breakthrough in medical research in Geneva, Switzerland.\",\n",
    "            \"entities\": {\n",
    "                \"organizations\": [\"World Health Organization\"],\n",
    "                \"locations\": [\"Geneva\", \"Switzerland\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Construct messages with few-shot examples\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Extract ALL {entity_types} from the text. Return ONLY a Python-style list.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Add few-shot examples\n",
    "    for example in few_shot_examples:\n",
    "        for entity_type, entities in example[\"entities\"].items():\n",
    "            if entity_type in entity_types:\n",
    "                messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": example[\"text\"]\n",
    "                })\n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": str(entities)\n",
    "                })\n",
    "    \n",
    "    # Add current text to extract entities from\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Extract ALL {entity_types} from this text: \\\"{text}\\\"\"\n",
    "    })\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model_name, \n",
    "        temperature=0.0, \n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Safely evaluate the response\n",
    "        response = chat_completion.choices[0].message.content.strip()\n",
    "        return eval(response)\n",
    "    except:\n",
    "        try:\n",
    "            # Handle both direct string response and JSON object \n",
    "            if isinstance(response, str):\n",
    "                return eval(response)\n",
    "            elif isinstance(response, dict):\n",
    "                return list(response.values())[0]  # Assumes first value is the list\n",
    "        except:\n",
    "            return []  # Fallback to empty list on parsing failure\n",
    "\n",
    "def impute_missing(\n",
    "    row: Dict[str, Any], \n",
    "    column_to_impute: str, \n",
    "    context_columns: List[str], \n",
    "    type_hint: Optional[str] = None, \n",
    "    id_columns: Optional[List[str]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generic missing value imputation with few-shot learning\n",
    "    Uses specified context columns and provides type-specific guidance\n",
    "    \"\"\"\n",
    "    # Few-shot examples for different imputation scenarios\n",
    "    few_shot_examples = {\n",
    "        \"numeric\": [\n",
    "            {\n",
    "                \"context\": \"age: 35, height: None, weight: 75\",\n",
    "                \"imputed_value\": \"180\"\n",
    "            },\n",
    "            {\n",
    "                \"context\": \"salary: 50000, bonus: None, years_experience: 5\",\n",
    "                \"imputed_value\": \"7500\"\n",
    "            }\n",
    "        ],\n",
    "        \"date\": [\n",
    "            {\n",
    "                \"context\": \"hire_date: 2020-01-15, term_start: None, last_promotion: 2022-06-01\",\n",
    "                \"imputed_value\": \"2020-07-01\"\n",
    "            }\n",
    "        ],\n",
    "        \"text\": [\n",
    "            {\n",
    "                \"context\": \"product_name: Smartphone X, description: None, category: Electronics\",\n",
    "                \"imputed_value\": \"High-performance smartphone with advanced features\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    result_row = row.copy()\n",
    "    \n",
    "    # Return early if no imputation needed\n",
    "    if row.get(column_to_impute) not in [None, \"NULL\", \"\", \"null\"] and not pd.isna(row.get(column_to_impute)):\n",
    "        return row[column_to_impute]\n",
    "    \n",
    "    # Filter context columns\n",
    "    id_columns = id_columns or []\n",
    "    available_context = [\n",
    "        (k, str(row[k])) for k in context_columns \n",
    "        if k in row and k != column_to_impute \n",
    "        and row[k] is not None and k not in id_columns\n",
    "    ]\n",
    "    \n",
    "    # Build context string\n",
    "    context_str = \"\\n\".join(f\"{k}: {v}\" for k, v in available_context)\n",
    "    if not context_str:\n",
    "        return row[column_to_impute]  # No context available\n",
    "    \n",
    "    # Construct messages with few-shot examples\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Impute missing value for '{column_to_impute}' based on context.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Add few-shot examples based on type hint\n",
    "    if type_hint and type_hint in few_shot_examples:\n",
    "        for example in few_shot_examples[type_hint]:\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Context: {example['context']}\"\n",
    "            })\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": example[\"imputed_value\"]\n",
    "            })\n",
    "    \n",
    "    # Determine type instruction\n",
    "    type_instruction = \"\"\n",
    "    if type_hint:\n",
    "        type_map = {\n",
    "            \"numeric\": \"Return only a number without symbols\",\n",
    "            \"date\": \"Use YYYY-MM-DD format\",\n",
    "            \"boolean\": \"Return 'true' or 'false'\",\n",
    "            \"text\": \"Return a string without quotes\"\n",
    "        }\n",
    "        type_instruction = f\"\\nIMPORTANT: {type_map.get(type_hint, '')}\"\n",
    "    \n",
    "    # Add current context\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Context: {context_str}{type_instruction}\"\n",
    "    })\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model_name, \n",
    "        temperature=0.0, \n",
    "        max_tokens=50\n",
    "    )\n",
    "    \n",
    "    result_row[column_to_impute] = chat_completion.choices[0].message.content.strip()\n",
    "    return result_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b398e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "cards_df = pd.read_csv(\"DB/card_games/cards_sample20.csv\")\n",
    "rulings_df = pd.read_csv(\"DB/card_games/ruling_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49739819",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts=pd.read_csv(r\"DB/codebase/posts20sample.csv\")\n",
    "users=pd.read_csv(r\"DB/codebase/users20sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26389a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentiment_analysis_001(cards: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Test Case ID: entity_classification_001\n",
    "    Calculates the sentiment score on cards' flavourtext and returns the negative ones\n",
    "    \"Which cards have flavor text with negative sentiment?\"\n",
    "    equivalent SQL:\n",
    "    SELECT name\n",
    "    FROM cards\n",
    "    WHERE flavorText IS NOT NULL\n",
    "    AND analyze_sentiment(flavorText) < 0;\n",
    "    \"\"\"\n",
    "    negative_cards = []\n",
    "    for _, card in cards.iterrows():\n",
    "        if pd.notna(card['flavorText']):\n",
    "            sentiment = analyze_sentiment(card['flavorText'])\n",
    "            if sentiment < 0:  # Negative sentiment\n",
    "                negative_cards.append(card['name'])\n",
    "    return negative_cards\n",
    "\n",
    "#print(test_sentiment_analysis_001(cards_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff003e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentiment_analysis_002(users: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Test Case ID: sentiment_analysis_002\n",
    "    Finds users with AboutMe descriptions having a negative sentiment (score below -0.5)\n",
    "    \"Find users with AboutMe descriptions having a negative sentiment (score below -0.5)\"\n",
    "    equivalent SQL:\n",
    "    SELECT Id\n",
    "    FROM users\n",
    "    WHERE AboutMe IS NOT NULL\n",
    "    AND analyze_sentiment(AboutMe) < -0.5;\n",
    "    \"\"\"\n",
    "    negative_users = []\n",
    "    for _, user in users.iterrows():\n",
    "        if pd.notna(user['AboutMe']):\n",
    "            sentiment = analyze_sentiment(user['AboutMe'])\n",
    "            if sentiment < -0.5:  # Highly negative sentiment\n",
    "                negative_users.append(str(user['Id']))\n",
    "    return negative_users\n",
    "\n",
    "#print(test_sentiment_analysis_002(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88612c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentiment_analysis_003(posts: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Test Case ID: sentiment_analysis_003\n",
    "    Identifies posts with highly positive sentiment (score > 0.7)\n",
    "    \"Identify posts with highly positive sentiment (score > 0.7)\"\n",
    "    equivalent SQL:\n",
    "    SELECT Id\n",
    "    FROM posts\n",
    "    WHERE Body IS NOT NULL\n",
    "    AND analyze_sentiment(Body) > 0.7;\n",
    "    \"\"\"\n",
    "    positive_posts = []\n",
    "    for _, post in posts.iterrows():\n",
    "        if pd.notna(post['Body']):\n",
    "            sentiment = analyze_sentiment(post['Body'])\n",
    "            if sentiment > 0.7:  # Highly positive sentiment\n",
    "                positive_posts.append(str(post['Id']))\n",
    "    return positive_posts\n",
    "#print(test_sentiment_analysis_003(posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4ce5cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentiment_analysis_004(users: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Test Case ID: sentiment_analysis_004\n",
    "    Finds user AboutMe descriptions with neutral sentiment (between -0.2 and 0.2)\n",
    "    \"Find user AboutMe descriptions with neutral sentiment (between -0.2 and 0.2)\"\n",
    "    equivalent SQL:\n",
    "    SELECT Id\n",
    "    FROM users\n",
    "    WHERE AboutMe IS NOT NULL\n",
    "    AND analyze_sentiment(AboutMe) BETWEEN -0.2 AND 0.2;\n",
    "    \"\"\"\n",
    "    neutral_users = []\n",
    "    for _, user in users.iterrows():\n",
    "        if pd.notna(user['AboutMe']):\n",
    "            sentiment = analyze_sentiment(user['AboutMe'])\n",
    "            if -0.2 <= sentiment <= 0.2:  # Neutral sentiment\n",
    "                neutral_users.append(str(user['Id']))\n",
    "    return neutral_users\n",
    "#print(test_sentiment_analysis_004(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "026daf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentiment_analysis_005(posts: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Test Case ID: sentiment_analysis_005\n",
    "    Identifies posts with slightly negative sentiment below 0\n",
    "    \"Identify posts with slightly negative sentiment below 0\"\n",
    "    equivalent SQL:\n",
    "    SELECT Id\n",
    "    FROM posts\n",
    "    WHERE Body IS NOT NULL\n",
    "    AND analyze_sentiment(Body) < -0.5;\n",
    "    \"\"\"\n",
    "    negative_posts = []\n",
    "    for _, post in posts.iterrows():\n",
    "        if pd.notna(post['Body']):\n",
    "            sentiment = analyze_sentiment(post['Body'])\n",
    "            if sentiment < 0:  # Highly negative sentiment\n",
    "                negative_posts.append(str(post['Id']))\n",
    "    return negative_posts\n",
    "#print(test_sentiment_analysis_005(posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ced7d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentiment_analysis_006(rulings: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Test Case ID: sentiment_analysis_006\n",
    "    Finds rulings with sentiment outside the neutral range (-0.2 to 0.2)\n",
    "    \"Find rulings with sentiment outside the neutral range (-0.2 to 0.2)\"\n",
    "    equivalent SQL:\n",
    "    SELECT id\n",
    "    FROM rulings\n",
    "    WHERE text IS NOT NULL\n",
    "    AND (analyze_sentiment(text) < -0.2 OR analyze_sentiment(text) > 0.2);\n",
    "    \"\"\"\n",
    "    distinctive_sentiment_rulings = []\n",
    "    for _, ruling in rulings.iterrows():\n",
    "        if pd.notna(ruling['text']):\n",
    "            sentiment = analyze_sentiment(ruling['text'])\n",
    "            if sentiment < -0.2 or sentiment > 0.2:  # Outside neutral range\n",
    "                distinctive_sentiment_rulings.append(str(ruling['id']))\n",
    "    return distinctive_sentiment_rulings\n",
    "#print(test_sentiment_analysis_006(rulings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "423e6b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_entity_classification_001(cards: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Test Case ID: entity_classification_001\n",
    "    Classifies the archetype of 'Twilight Prophet' based on text using\n",
    "    \"What is the archetype of Twilight Prophet?\"\n",
    "    equivalent SQL:\n",
    "    SELECT \n",
    "    'Twilight Prophet' AS card_name,\n",
    "    classify_entity(\n",
    "        text, \n",
    "        ARRAY['Aggro', 'Control', 'Combo']  -- Array of possible archetypes\n",
    "    ) AS archetype\n",
    "    FROM cards\n",
    "    WHERE name = 'Twilight Prophet'\n",
    "    LIMIT 1; \n",
    "    \"\"\"\n",
    "    # Get the specific card data\n",
    "    card = cards[cards['name'] == 'Twilight Prophet'].iloc[0]\n",
    "    archetype=classify_entity(card['text'],[\"Aggro\",\"Control\",\"Combo\"])\n",
    "    return archetype\n",
    "\n",
    "#print(test_entity_classification_001(cards_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63e3debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_entity_recognition_001(cards: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Test Case ID: entity_recognition_001\n",
    "    Identifies people or creature entities in a text and returns all the cards that present identified entities in their flavour text\n",
    "    Which cards mention creatures or people in their flavor text?\n",
    "    equivalent SQL:\n",
    "    SELECT DISTINCT name\n",
    "    FROM cards\n",
    "    WHERE flavorText IS NOT NULL\n",
    "    AND extract_entities(flavorText, ARRAY['PERSON','CREATURE']::text[]) IS NOT NULL;\n",
    "    \n",
    "    \"\"\"\n",
    "    entity_cards = []\n",
    "    for _, card in cards.iterrows():\n",
    "        if pd.notna(card['flavorText']):\n",
    "            entities = extract_entities(card['flavorText'], [\"PERSON\",\"CREATURE\"])\n",
    "            if entities:  # Found mentions of people/creatures\n",
    "                entity_cards.append(card['name'])\n",
    "    return entity_cards\n",
    "\n",
    "#print(test_entity_recognition_001(cards_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abd052f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_schema_alignment_001(cards: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Test Case ID: schema_alignment_001  \n",
    "    Map card properties to Schema.org types.\n",
    "    equivalent SQL:\n",
    "    SELECT \n",
    "        name,\n",
    "        align_to_schema(\n",
    "            JSON_BUILD_OBJECT(\n",
    "                'name', name,\n",
    "                'flavorText', flavorText,\n",
    "                'text', text,\n",
    "                'type', type\n",
    "            ),\n",
    "            'Game'\n",
    "        ) AS mapped_properties\n",
    "    FROM cards\n",
    "    WHERE name = 'Lady Sun';\n",
    "      \n",
    "    \"\"\"\n",
    "    aligned = []\n",
    "    for _, card in cards.iterrows():\n",
    "        if card['name'] == \"Lady Sun\":  # From test case\n",
    "            mapped = align_to_schema({\n",
    "                \"name\": card['name'],\n",
    "                \"flavorText\": card['flavorText'],\n",
    "                \"text\": card['text'],\n",
    "                \"type\": card['type']\n",
    "            }, \"Game\")\n",
    "            aligned.append({\n",
    "                \"name\": card['name'],\n",
    "                \"mapped_properties\": mapped\n",
    "            })\n",
    "    return aligned\n",
    "#print(test_schema_alignment_001(cards_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be693abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_imputation_missing_001(cards: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Test Case ID: imputation_missing_001\n",
    "    Calls the llm to try and fill a specific column value based on external knowledge and the other columns values \n",
    "    \"Fill in the missing toughness value of creatures based on other card fields.\"\n",
    "    equivalent SQL:\n",
    "    WITH creature_cards AS (\n",
    "    SELECT *\n",
    "    FROM cards\n",
    "    WHERE LOWER(type) LIKE '%creature%'\n",
    "    ),\n",
    "    cards_to_impute AS (\n",
    "    SELECT *\n",
    "    FROM creature_cards\n",
    "    WHERE toughness IS NULL \n",
    "        OR toughness = '' \n",
    "        OR LOWER(toughness) = 'null'\n",
    "    )\n",
    "    SELECT \n",
    "    name,\n",
    "    impute_missing(\n",
    "        JSON_BUILD_OBJECT(\n",
    "        'name', name,\n",
    "        'type', type,\n",
    "        'power', power,\n",
    "        'text', text\n",
    "        ),\n",
    "        'toughness',\n",
    "        ARRAY['name', 'type', 'power', 'text'],\n",
    "        'numeric'\n",
    "    ) AS imputed_toughness\n",
    "    FROM cards_to_impute;\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get the creature type cards\n",
    "    creature_cards = cards_df[cards_df['type'].str.contains('Creature', case=False, na=False)].copy()\n",
    "    # List of cards to modify \n",
    "    cards_to_update = [\"Civic Wayfinder\", \"Dream Stalker\", \"Esper Sojourners\"]\n",
    "\n",
    "    # Update toughness to None/NULL for those specific cards\n",
    "    creature_cards.loc[creature_cards['name'].isin(cards_to_update), 'toughness'] = None\n",
    "    imputed_cards=[]\n",
    "    creature_cards=creature_cards.to_dict('records')\n",
    "    for card in creature_cards:\n",
    "        if pd.isna(card[\"toughness\"]) or card[\"toughness\"] in [None, \"NULL\", \"\", \"null\"]:\n",
    "            imputed_cards.append({\"name\":card[\"name\"],\"imputed_toughness\":impute_missing(\n",
    "        row=card,\n",
    "        column_to_impute=\"toughness\",\n",
    "        context_columns=[\"name\", \"type\", \"power\", \"text\"],\n",
    "        type_hint=\"numeric\")[\"toughness\"]})\n",
    "    return(imputed_cards)\n",
    "    \n",
    "#print(test_imputation_missing_001(cards_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f7ab784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_summarization_001(cards: pd.DataFrame, rulings: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Test Case ID: summarization_001\n",
    "    Calls the llm to summarize a text\n",
    "    \"Summarize rulings for 'Esper Sojourners'.\"\n",
    "    equivalent SQL:\n",
    "    WITH card_uuid AS (\n",
    "    SELECT uuid\n",
    "    FROM cards\n",
    "    WHERE name = 'Esper Sojourners'\n",
    "    ),\n",
    "    combined_rulings AS (\n",
    "    SELECT \n",
    "        c.name,\n",
    "        STRING_AGG(r.text, E'\\n' ORDER BY r.text) AS rulings_text\n",
    "    FROM card_uuid cu\n",
    "    JOIN cards c ON c.uuid = cu.uuid\n",
    "    JOIN rulings r ON r.uuid = cu.uuid\n",
    "    GROUP BY c.name\n",
    "    )\n",
    "    SELECT \n",
    "    name,\n",
    "    summarize(rulings_text, 100) AS summary\n",
    "    FROM combined_rulings;\n",
    "    \n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    card_name = \"Esper Sojourners\"\n",
    "    card_uuid = cards[cards['name'] == card_name]['uuid'].iloc[0]\n",
    "    card_rulings = rulings[rulings['uuid'] == card_uuid]\n",
    "    \n",
    "    if not card_rulings.empty:\n",
    "        combined_rulings = \"\\n\".join(card_rulings['text'])\n",
    "        summary = summarize(combined_rulings, 100)\n",
    "        summaries.append({\n",
    "            \"name\": card_name,\n",
    "            \"summary\": summary\n",
    "        })\n",
    "    return summaries\n",
    "\n",
    "#print(test_summarization_001(cards_df,rulings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce39f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classification_002(cards: pd.DataFrame, rulings: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Test Case ID: classification_002\n",
    "    Identifies cards with rulings classified as \"damage_prevention\" \n",
    "    \"Which cards have rulings that mention 'damage prevention'?\"\n",
    "    equivalent SQL:\n",
    "    SELECT DISTINCT c.name\n",
    "    FROM rulings r\n",
    "    JOIN cards c ON r.uuid = c.uuid\n",
    "    WHERE classify_entity(r.text, ARRAY['damage_prevention', 'other']) = 'damage_prevention';\n",
    "    \n",
    "    \"\"\"\n",
    "    damage_prevention_cards = []\n",
    "    \n",
    "    for _, ruling in rulings.iterrows():\n",
    "        # Classify the ruling text\n",
    "        classification = classify_entity(\n",
    "            text=ruling['text'],\n",
    "            classes=[\"damage_prevention\", \"other\"],\n",
    "        )\n",
    "        \n",
    "        if classification == \"damage_prevention\":\n",
    "            card = cards[cards['uuid'] == ruling['uuid']].iloc[0]\n",
    "            damage_prevention_cards.append(card['name'])\n",
    "    \n",
    "    return list(set(damage_prevention_cards))\n",
    "#print(test_classification_002(cards_df,rulings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b54f6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_entity_recognition_002(cards: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Test Case ID: entity_recognition_002\n",
    "    call llm to analyze the text to identify cards that give a buff and count how many identified cards for each type\n",
    "    How many cards of each type have text that inplies giving a buff?\n",
    "    equivalent SQL:\n",
    "    SELECT \n",
    "        type,\n",
    "        COUNT(CASE WHEN extract_entities(text, ARRAY['buff', 'stat increase']) IS NOT NULL THEN 1 END) AS buff_count,\n",
    "        COUNT(*) AS total_cards\n",
    "    FROM cards\n",
    "    WHERE text IS NOT NULL\n",
    "    GROUP BY type;\n",
    "    \n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Group by card type and process each group\n",
    "    for card_type, group in cards.groupby('type'):\n",
    "        buff_count = 0\n",
    "        \n",
    "        for text in group['text'].dropna():  # Skip NaN values\n",
    "            extracted = extract_entities(text, [\"buff\", \"stat increase\"])\n",
    "            if extracted:\n",
    "                buff_count += 1\n",
    "                \n",
    "        \n",
    "        results.append({\n",
    "            \"type\": card_type,\n",
    "            \"buff_count\": buff_count,\n",
    "            \"total_cards\": len(group)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "#print(test_entity_recognition_002(cards_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "083437fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analyze_sentiment_summarize_001(cards: pd.DataFrame, rulings: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Test Case ID: analyze_sentiment_summarize_001\n",
    "    calls an llm to assing a sentiment score to flavour text of the cards,\n",
    "    than takes the cards with positive score(>0.5)\n",
    "    and finds those wid rulings, finally calls again the llm to produce\n",
    "    a summary of said rulings\n",
    "    \"Cards with positive flavor text sentiment and complex rulings.\"\n",
    "    equivalent SQL:\n",
    "    SELECT\n",
    "    c.name,\n",
    "    summarize(string_agg(r.text, E'\\n'), 100) AS summary,\n",
    "    'positive' AS sentiment\n",
    "    FROM cards c\n",
    "    JOIN rulings r ON c.uuid = r.uuid\n",
    "    WHERE c.flavorText IS NOT NULL\n",
    "    AND analyze_sentiment(c.flavorText) > 0.5\n",
    "    GROUP BY c.name;\n",
    "    \n",
    "    \"\"\"\n",
    "    complex_cards = []\n",
    "    for _, card in cards.iterrows():\n",
    "        if pd.notna(card['flavorText']):\n",
    "            sentiment = analyze_sentiment(card['flavorText'])\n",
    "            if sentiment > 0.5:\n",
    "                card_rulings = rulings[rulings['uuid'] == card['uuid']]\n",
    "                if not card_rulings.empty:\n",
    "                    combined_rulings = \"\\n\".join(card_rulings['text'])\n",
    "                    summary = summarize(combined_rulings, 100)\n",
    "                    complex_cards.append({\n",
    "                        \"name\": card['name'],\n",
    "                        \"summary\": summary,\n",
    "                        \"sentiment\": \"positive\"\n",
    "                    })\n",
    "    return complex_cards\n",
    "#print(test_analyze_sentiment_summarize_001(cards_df,rulings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29979584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_entity_impute_classify_001(cards: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Test Case ID: analyze_sentiment_summarize_001\n",
    "    For creature cards with missing rarity:\n",
    "    finds the cards with missing rarity  values, calls the llm to predict it and\n",
    "    classify archetype\n",
    "    \"For creature cards with missing rarity, predict the rarity based on their stats and classify them by their gameplay archetype.\"\n",
    "    equivalent SQL:\n",
    "    WITH missing_rarity_creatures AS (\n",
    "    SELECT *\n",
    "    FROM cards\n",
    "    WHERE LOWER(type) LIKE '%creature%'\n",
    "        AND rarity IS NULL\n",
    "    ),\n",
    "    processed_cards AS (\n",
    "    SELECT\n",
    "        name,\n",
    "        impute_missing(\n",
    "        JSON_BUILD_OBJECT(\n",
    "            'name', name,\n",
    "            'type', type,\n",
    "            'power', power,\n",
    "            'toughness', toughness,\n",
    "            'text', text,\n",
    "            'manaCost', manaCost\n",
    "        ),\n",
    "        'rarity',\n",
    "        ARRAY['name', 'type', 'power', 'toughness', 'text', 'manaCost'],\n",
    "        'text'\n",
    "        )->>'rarity' AS imputed_rarity,\n",
    "        classify_entity(\n",
    "        CONCAT(\n",
    "            'Card: ', name, '\\n',\n",
    "            'Stats: ', power, '/', toughness, ' (MV: ', manaCost, ')\\n',\n",
    "            'Text: ', COALESCE(text, '')\n",
    "        ),\n",
    "        ARRAY['aggro', 'control', 'combo', 'midrange', 'hybrid']\n",
    "        ) AS archetype\n",
    "    FROM missing_rarity_creatures\n",
    "    )\n",
    "    SELECT\n",
    "    name,\n",
    "    imputed_rarity,\n",
    "    LOWER(archetype) AS archetype\n",
    "    FROM processed_cards;\n",
    "    \n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Force rarity=None for our test cases\n",
    "    cards.loc[cards['name'].isin([\n",
    "        \"Dream Stalker\", \n",
    "        \"Ostiary Thrull\", \n",
    "        \"Crested Sunmare\"\n",
    "    ]), 'rarity'] = None\n",
    "    \n",
    "    # Filter creatures with missing rarity\n",
    "    creatures = cards[\n",
    "        (cards['type'].str.contains('Creature', case=False, na=False)) & \n",
    "        (cards['rarity'].isna())\n",
    "    ].copy()\n",
    "    \n",
    "    for _, card in creatures.iterrows():\n",
    "        # Convert card data to dict for UDF processing\n",
    "        card_data = card.to_dict()\n",
    "        \n",
    "        # Impute missing rarity\n",
    "        imputed_card = impute_missing(\n",
    "            row=card_data,\n",
    "            column_to_impute=\"rarity\",\n",
    "            context_columns=[\"name\", \"type\", \"power\", \"toughness\", \"text\", \"manaCost\"],\n",
    "            type_hint=\"text\"\n",
    "        )\n",
    "        \n",
    "        #Classify archetype\n",
    "        archetype = classify_entity(\n",
    "            text=f\"\"\"\n",
    "            Card: {card['name']}\n",
    "            Stats: {card['power']}/{card['toughness']} (MV: {card['manaCost']})\n",
    "            Text: {card['text']}\n",
    "            \"\"\",\n",
    "            classes=[\"aggro\", \"control\", \"combo\", \"midrange\", \"hybrid\"],\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"name\": card['name'],\n",
    "            \"imputed_rarity\": imputed_card[\"rarity\"],\n",
    "            \"archetype\": archetype.lower()\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "#print(test_entity_impute_classify_001(cards_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "705d4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_functions = {\n",
    "        \"sentiment_analysis_001\": lambda: test_sentiment_analysis_001(cards_df),\n",
    "        \"sentiment_analysis_002\": lambda: test_sentiment_analysis_002(users),\n",
    "        \"sentiment_analysis_003\": lambda: test_sentiment_analysis_003(posts),\n",
    "        \"sentiment_analysis_004\": lambda: test_sentiment_analysis_004(users),\n",
    "        \"sentiment_analysis_005\": lambda: test_sentiment_analysis_005(posts),\n",
    "        \"sentiment_analysis_006\": lambda: test_sentiment_analysis_006(rulings_df),\n",
    "        \"entity_classification_001\": lambda: test_entity_classification_001(cards_df),\n",
    "        \"classification_002\": lambda: test_classification_002(cards_df,rulings_df),\n",
    "        \"entity_recognition_001\": lambda: test_entity_recognition_001(cards_df),\n",
    "        \"entity_recognition_002\": lambda: test_entity_recognition_002(cards_df),\n",
    "        \"schema_alignment_001\": lambda: test_schema_alignment_001(cards_df),\n",
    "        \"imputation_missing_001\": lambda: test_imputation_missing_001(cards_df),\n",
    "        \"summarization_001\": lambda: test_summarization_001(cards_df, rulings_df),\n",
    "        \"analyze_sentiment_summarize_001\": lambda: test_analyze_sentiment_summarize_001(cards_df,rulings_df),\n",
    "        \"entity_impute_classify_001\": lambda: test_entity_impute_classify_001(cards_df),\n",
    "    }\n",
    "test_results = []\n",
    "# Load test cases\n",
    "import json\n",
    "\n",
    "# Load first JSON file\n",
    "with open('sentiment_analysis_examples.json', 'r') as f1:\n",
    "    data1 = json.load(f1)\n",
    "\n",
    "# Load second JSON file\n",
    "with open(\"udf_examples_part1.json\", 'r') as f2:\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "# Join the data (assuming both are lists)\n",
    "test_cases = data1 + data2\n",
    "\n",
    "for case in test_cases:\n",
    "    test_id = case['unique_id']\n",
    "    if test_id in test_functions:\n",
    "        actual = test_functions[test_id]()\n",
    "        passed = actual == case['expected_result']\n",
    "        test_results.append({\n",
    "            \"test_id\": test_id,\n",
    "            \"question\": case['question'],\n",
    "            \"expected\": case['expected_result'],\n",
    "            \"actual\": actual,\n",
    "            \"passed\": passed\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc4c395d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'test_id': 'sentiment_analysis_001',\n",
       "  'question': 'Which cards have flavor text with negative sentiment?',\n",
       "  'expected': ['Ostiary Thrull'],\n",
       "  'actual': ['Ostiary Thrull'],\n",
       "  'passed': True},\n",
       " {'test_id': 'sentiment_analysis_002',\n",
       "  'question': 'Find users with AboutMe descriptions having a negative sentiment (score below -0.5)',\n",
       "  'expected': ['2609', '2269'],\n",
       "  'actual': ['2269'],\n",
       "  'passed': False},\n",
       " {'test_id': 'sentiment_analysis_003',\n",
       "  'question': 'Identify posts with highly positive sentiment (score > 0.7)',\n",
       "  'expected': ['7153', '46149', '7953'],\n",
       "  'actual': [],\n",
       "  'passed': False},\n",
       " {'test_id': 'sentiment_analysis_004',\n",
       "  'question': 'Find user AboutMe descriptions with neutral sentiment (between 0.2 ,-0.2)',\n",
       "  'expected': ['41203', '52599'],\n",
       "  'actual': ['41203', '52599', '7547', '31765'],\n",
       "  'passed': False},\n",
       " {'test_id': 'sentiment_analysis_005',\n",
       "  'question': 'Identify posts with slightly negative sentiment below -0.0',\n",
       "  'expected': ['107797', '60435', '61283', '9521', '11566'],\n",
       "  'actual': [],\n",
       "  'passed': False},\n",
       " {'test_id': 'sentiment_analysis_006',\n",
       "  'question': 'Find rulings with sentiment outside the neutral range (-0.2 to 0.2)',\n",
       "  'expected': ['Ids of rulings with distinctive emotional tone',\n",
       "   '38019',\n",
       "   '37243',\n",
       "   '61996',\n",
       "   '84807'],\n",
       "  'actual': [],\n",
       "  'passed': False},\n",
       " {'test_id': 'entity_classification_001',\n",
       "  'question': 'What is the archetype of Twilight Prophet?',\n",
       "  'expected': [{'name': 'Twilight Prophet', 'archetype': 'control'}],\n",
       "  'actual': 'Control',\n",
       "  'passed': False},\n",
       " {'test_id': 'entity_recognition_001',\n",
       "  'question': 'Which cards mention creatures or people in their flavor text?',\n",
       "  'expected': ['Civic Wayfinder',\n",
       "   'Ostiary Thrull',\n",
       "   'Lady Sun',\n",
       "   'Revolutionary Rebuff'],\n",
       "  'actual': ['Crested Sunmare',\n",
       "   'Civic Wayfinder',\n",
       "   'Helm of the Gods',\n",
       "   'Ancestral Memories',\n",
       "   'Lady Sun'],\n",
       "  'passed': False},\n",
       " {'test_id': 'schema_alignment_001',\n",
       "  'question': 'Map the card properties to Schema.org types and properties.',\n",
       "  'expected': [{'name': 'Lady Sun',\n",
       "    'mapped_properties': {'name': 'name',\n",
       "     'flavorText': 'description',\n",
       "     'text': 'text',\n",
       "     'type': 'additionalType'}}],\n",
       "  'actual': [{'name': 'Lady Sun',\n",
       "    'mapped_properties': {'name': 'name',\n",
       "     'flavorText': 'description',\n",
       "     'text': 'gameplayDescription',\n",
       "     'type': 'gameItemVariant'}}],\n",
       "  'passed': False},\n",
       " {'test_id': 'imputation_missing_001',\n",
       "  'question': 'Fill in the missing toughness value of creatures based on other card fields.',\n",
       "  'expected': [{'name': 'Civic Wayfinder', 'imputed_toughness': '2'},\n",
       "   {'name': 'Dream Stalker', 'imputed_toughness': '5'},\n",
       "   {'name': 'Esper Sojourners', 'imputed_toughness': '3'}],\n",
       "  'actual': [{'name': 'Esper Sojourners', 'imputed_toughness': '2'},\n",
       "   {'name': 'Dream Stalker', 'imputed_toughness': '2'},\n",
       "   {'name': 'Civic Wayfinder', 'imputed_toughness': '3'}],\n",
       "  'passed': False},\n",
       " {'test_id': 'summarization_001',\n",
       "  'question': \"Summarize the rulings for the card 'Esper Sojourners'.\",\n",
       "  'expected': [{'name': 'Esper Sojourners',\n",
       "    'summary': 'Cycling is an activated ability not affected by suppression, and it triggers another ability when used.'}],\n",
       "  'actual': [{'name': 'Esper Sojourners',\n",
       "    'summary': 'Cycling interacts with activated ability effects, not spell effects.'}],\n",
       "  'passed': False},\n",
       " {'test_id': 'analyze_sentiment_summarize_001',\n",
       "  'question': 'Which cards with positive flavor text sentiment also have complex rulings? Summarize the rulings.',\n",
       "  'expected': [{'name': 'Crested SunmareCrested Sunmare',\n",
       "    'summary': 'Damage stays until cleanup. If Sunmare leaves after lethal Horse damage, it dies. Needs life gain before end step, not during.',\n",
       "    'sentiment': 'positive'},\n",
       "   {'name': 'Helm of the Gods',\n",
       "    'summary': \"You control Auras cast on opponent's permanents. They count for Helm of the Gods' bonus.\",\n",
       "    'sentiment': 'positive'}],\n",
       "  'actual': [{'name': 'Crested Sunmare',\n",
       "    'summary': \"Crested Sunmare's ability triggers if you gained life earlier in the turn, not during the end step.\",\n",
       "    'sentiment': 'positive'},\n",
       "   {'name': 'Helm of the Gods',\n",
       "    'summary': \"You control Aura spells targeting opponents' permanents.\",\n",
       "    'sentiment': 'positive'}],\n",
       "  'passed': False},\n",
       " {'test_id': 'entity_recognition_002',\n",
       "  'question': 'How many cards of each type have text that includes giving a buff?',\n",
       "  'expected': [{'type': 'Artifact Creature — Horse',\n",
       "    'buff_count': 0,\n",
       "    'total_cards': 1},\n",
       "   {'type': 'Artifact Creature — Vedalken Wizard',\n",
       "    'buff_count': 0,\n",
       "    'total_cards': 1},\n",
       "   {'type': 'Artifact — Equipment', 'buff_count': 1, 'total_cards': 1},\n",
       "   {'type': 'Creature — Elf Druid Warrior', 'buff_count': 0, 'total_cards': 1},\n",
       "   {'type': 'Creature — Horse', 'buff_count': 1, 'total_cards': 1},\n",
       "   {'type': 'Creature — Illusion', 'buff_count': 0, 'total_cards': 1},\n",
       "   {'type': 'Creature — Merfolk Soldier', 'buff_count': 0, 'total_cards': 1},\n",
       "   {'type': 'Creature — Thrull', 'buff_count': 0, 'total_cards': 1},\n",
       "   {'type': 'Creature — Vampire Cleric', 'buff_count': 0, 'total_cards': 1},\n",
       "   {'type': 'Enchantment', 'buff_count': 1, 'total_cards': 2},\n",
       "   {'type': 'Instant', 'buff_count': 1, 'total_cards': 2},\n",
       "   {'type': 'Land', 'buff_count': 0, 'total_cards': 3},\n",
       "   {'type': 'Legendary Creature — Human Advisor',\n",
       "    'buff_count': 0,\n",
       "    'total_cards': 1},\n",
       "   {'type': 'Snow Sorcery', 'buff_count': 1, 'total_cards': 1},\n",
       "   {'type': 'Sorcery', 'buff_count': 0, 'total_cards': 2}],\n",
       "  'actual': [{'type': 'Artifact Creature — Horse',\n",
       "    'buff_count': 1,\n",
       "    'total_cards': 1},\n",
       "   {'type': 'Artifact Creature — Vedalken Wizard',\n",
       "    'buff_count': 0,\n",
       "    'total_cards': 1},\n",
       "   {'type': 'Artifact — Equipment', 'buff_count': 1, 'total_cards': 1},\n",
       "   {'type': 'Creature — Elf Druid Warrior', 'buff_count': 0, 'total_cards': 1},\n",
       "   {'type': 'Creature — Horse', 'buff_count': 1, 'total_cards': 1},\n",
       "   {'type': 'Creature — Illusion', 'buff_count': 0, 'total_cards': 1},\n",
       "   {'type': 'Creature — Merfolk Soldier', 'buff_count': 0, 'total_cards': 1},\n",
       "   {'type': 'Creature — Thrull', 'buff_count': 0, 'total_cards': 1},\n",
       "   {'type': 'Creature — Vampire Cleric', 'buff_count': 0, 'total_cards': 1},\n",
       "   {'type': 'Enchantment', 'buff_count': 2, 'total_cards': 2},\n",
       "   {'type': 'Instant', 'buff_count': 1, 'total_cards': 2},\n",
       "   {'type': 'Land', 'buff_count': 1, 'total_cards': 3},\n",
       "   {'type': 'Legendary Creature — Human Advisor',\n",
       "    'buff_count': 0,\n",
       "    'total_cards': 1},\n",
       "   {'type': 'Snow Sorcery', 'buff_count': 1, 'total_cards': 1},\n",
       "   {'type': 'Sorcery', 'buff_count': 0, 'total_cards': 2}],\n",
       "  'passed': False},\n",
       " {'test_id': 'classification_002',\n",
       "  'question': \"Which cards have rulings that mention 'damage prevention'?\",\n",
       "  'expected': ['Divine Deflection', 'Blizzard Brawl'],\n",
       "  'actual': ['Lady Sun',\n",
       "   'Esper Sojourners',\n",
       "   'Blizzard Brawl',\n",
       "   'Twilight Prophet',\n",
       "   'Crested Sunmare',\n",
       "   'Helm of the Gods',\n",
       "   'Illusions of Grandeur'],\n",
       "  'passed': False},\n",
       " {'test_id': 'entity_impute_classify_001',\n",
       "  'question': 'For creature cards with missing rarity, predict the rarity based on their stats and classify them by their gameplay archetype.',\n",
       "  'expected': [{'name': 'Dream Stalker',\n",
       "    'imputed_rarity': 'Uncommon',\n",
       "    'archetype': 'control'},\n",
       "   {'name': 'Ostiary Thrull',\n",
       "    'imputed_rarity': 'Common',\n",
       "    'archetype': 'control'},\n",
       "   {'name': 'Crested Sunmare',\n",
       "    'imputed_rarity': 'Rare',\n",
       "    'archetype': 'hybrid'}],\n",
       "  'actual': [{'name': 'Crested Sunmare',\n",
       "    'imputed_rarity': 'Rare',\n",
       "    'archetype': 'midrange'},\n",
       "   {'name': 'Dream Stalker', 'imputed_rarity': 'Rare', 'archetype': 'control'},\n",
       "   {'name': 'Ostiary Thrull',\n",
       "    'imputed_rarity': 'Uncommon',\n",
       "    'archetype': 'control'}],\n",
       "  'passed': False}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c948f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 6.67%\n",
      "\n",
      "Accuracy by Test Type:\n",
      "        test_type  accuracy  test_count\n",
      "4   summarization  0.833333           1\n",
      "5         analyze  0.583333           1\n",
      "1          entity  0.511111           4\n",
      "2          schema  0.500000           1\n",
      "3      imputation  0.500000           1\n",
      "0       sentiment  0.388889           6\n",
      "6  classification  0.222222           1\n",
      "\n",
      "Individual Test Accuracies:\n",
      "                            test_id  accuracy       test_type\n",
      "11  analyze_sentiment_summarize_001  0.583333         analyze\n",
      "13               classification_002  0.222222  classification\n",
      "12           entity_recognition_002  0.933333          entity\n",
      "14       entity_impute_classify_001  0.666667          entity\n",
      "7            entity_recognition_001  0.444444          entity\n",
      "6         entity_classification_001  0.000000          entity\n",
      "9            imputation_missing_001  0.500000      imputation\n",
      "8              schema_alignment_001  0.500000          schema\n",
      "0            sentiment_analysis_001  1.000000       sentiment\n",
      "1            sentiment_analysis_002  0.666667       sentiment\n",
      "3            sentiment_analysis_004  0.666667       sentiment\n",
      "2            sentiment_analysis_003  0.000000       sentiment\n",
      "4            sentiment_analysis_005  0.000000       sentiment\n",
      "5            sentiment_analysis_006  0.000000       sentiment\n",
      "10                summarization_001  0.833333   summarization\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'overall_accuracy': 0.06666666666666667,\n",
       " 'test_accuracies': {'sentiment_analysis_001': 1.0,\n",
       "  'sentiment_analysis_002': 0.6666666666666666,\n",
       "  'sentiment_analysis_003': 0,\n",
       "  'sentiment_analysis_004': 0.6666666666666666,\n",
       "  'sentiment_analysis_005': 0,\n",
       "  'sentiment_analysis_006': 0,\n",
       "  'entity_classification_001': 0.0,\n",
       "  'entity_recognition_001': 0.4444444444444445,\n",
       "  'schema_alignment_001': 0.5,\n",
       "  'imputation_missing_001': 0.5,\n",
       "  'summarization_001': 0.8333333333333334,\n",
       "  'analyze_sentiment_summarize_001': 0.5833333333333334,\n",
       "  'entity_recognition_002': 0.9333333333333332,\n",
       "  'classification_002': 0.22222222222222224,\n",
       "  'entity_impute_classify_001': 0.6666666666666666},\n",
       " 'type_accuracies': {'sentiment': 0.38888888888888884,\n",
       "  'entity': 0.5111111111111111,\n",
       "  'schema': 0.5,\n",
       "  'imputation': 0.5,\n",
       "  'summarization': 0.8333333333333334,\n",
       "  'analyze': 0.5833333333333334,\n",
       "  'classification': 0.22222222222222224}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load the test results\n",
    "data = test_results\n",
    "\n",
    "# Initialize results storage\n",
    "results = {}\n",
    "\n",
    "# Overall accuracy based on passed flag\n",
    "overall_accuracy = sum(1 for test in data if test['passed']) / len(data)\n",
    "results['overall_accuracy'] = overall_accuracy\n",
    "\n",
    "# Calculate individual test accuracies\n",
    "test_accuracies = {}\n",
    "\n",
    "# Group tests by type\n",
    "test_groups = {}\n",
    "for test in data:\n",
    "    test_type = test['test_id'].split('_')[0]\n",
    "    if test_type not in test_groups:\n",
    "        test_groups[test_type] = []\n",
    "    test_groups[test_type].append(test)\n",
    "\n",
    "# Function to calculate accuracy for list comparison tests\n",
    "def calculate_list_accuracy(expected, actual):\n",
    "    if not expected:\n",
    "        return 1.0 if not actual else 0.0\n",
    "    \n",
    "    # Calculate precision, recall, and F1\n",
    "    true_positives = sum(1 for item in actual if item in expected)\n",
    "    precision = true_positives / len(actual) if actual else 0\n",
    "    recall = true_positives / len(expected) if expected else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Function to calculate RAGAS-inspired metrics for summarization tests\n",
    "def calculate_ragas_accuracy(expected, actual):\n",
    "    \n",
    "    # Content similarity (how much of the expected content is in the actual)\n",
    "    # In a full implementation, this would use text embedding similarity or token overlap\n",
    "    content_similarity = 0.0\n",
    "    \n",
    "    # Simple implementation - check if key points are mentioned\n",
    "    if isinstance(expected, list) and isinstance(actual, list):\n",
    "        # Handle case where both are lists of dictionaries\n",
    "        if all(isinstance(e, dict) for e in expected) and all(isinstance(a, dict) for a in actual):\n",
    "            # Match by name\n",
    "            for exp_item in expected:\n",
    "                for act_item in actual:\n",
    "                    if exp_item.get('name') == act_item.get('name'):\n",
    "                        # Compare summaries if both exist\n",
    "                        if 'summary' in exp_item and 'summary' in act_item:\n",
    "                            # Simple string comparison (in real RAGAS, this would be semantic similarity)\n",
    "                            exp_summary = exp_item['summary'].lower()\n",
    "                            act_summary = act_item['summary'].lower()\n",
    "                            \n",
    "                            # Check for key phrases (simplified semantic similarity)\n",
    "                            key_phrases = [phrase.strip() for phrase in exp_summary.split('.') if phrase.strip()]\n",
    "                            matched_phrases = sum(1 for phrase in key_phrases if any(keyword in act_summary for keyword in phrase.split()))\n",
    "                            \n",
    "                            local_similarity = matched_phrases / len(key_phrases) if key_phrases else 0\n",
    "                            content_similarity += local_similarity\n",
    "            \n",
    "            content_similarity = content_similarity / len(expected) if expected else 0\n",
    "    \n",
    "    # Factual consistency (simplified)\n",
    "    factual_consistency = 0.7  # Assume 70% factual consistency for simplicity\n",
    "    \n",
    "    # Answer relevance (simplified)\n",
    "    answer_relevance = 0.8  # Assume 80% relevance for simplicity\n",
    "    \n",
    "    # RAGAS score (simplified composite)\n",
    "    ragas_score = (content_similarity + factual_consistency + answer_relevance) / 3\n",
    "    \n",
    "    return ragas_score\n",
    "\n",
    "# Function to calculate accuracy for object comparison tests\n",
    "def calculate_object_accuracy(expected, actual):\n",
    "    if isinstance(expected, dict) and isinstance(actual, dict):\n",
    "        # Compare dictionaries\n",
    "        common_keys = set(expected.keys()) & set(actual.keys()) \n",
    "        matching_values = sum(1 for k in common_keys if expected[k] == actual[k])\n",
    "        total_keys = len(set(expected.keys()) | set(actual.keys()))\n",
    "        return matching_values / total_keys if total_keys > 0 else 0\n",
    "    \n",
    "    elif isinstance(expected, list) and isinstance(actual, list):\n",
    "        # Handle lists of dictionaries (special case for some tests)\n",
    "        if all(isinstance(e, dict) for e in expected) and all(isinstance(a, dict) for a in actual):\n",
    "            # Entity recognition test with object lists\n",
    "            accuracy_sum = 0\n",
    "            count = 0\n",
    "            \n",
    "            # Try to match each expected item with actual items\n",
    "            for exp_item in expected:\n",
    "                best_match_accuracy = 0\n",
    "                for act_item in actual:\n",
    "                    # For simple objects, try to match by key\n",
    "                    if 'type' in exp_item and 'type' in act_item and exp_item['type'] == act_item['type']:\n",
    "                        item_accuracy = calculate_object_accuracy(exp_item, act_item)\n",
    "                        best_match_accuracy = max(best_match_accuracy, item_accuracy)\n",
    "                    elif 'name' in exp_item and 'name' in act_item and exp_item['name'] == act_item['name']:\n",
    "                        item_accuracy = calculate_object_accuracy(exp_item, act_item)\n",
    "                        best_match_accuracy = max(best_match_accuracy, item_accuracy)\n",
    "                \n",
    "                accuracy_sum += best_match_accuracy\n",
    "                count += 1\n",
    "            \n",
    "            return accuracy_sum / count if count > 0 else 0\n",
    "        else:\n",
    "            # Simple list comparison\n",
    "            return calculate_list_accuracy(expected, actual)\n",
    "    \n",
    "    # Fallback for string comparison\n",
    "    return 1.0 if expected == actual else 0.0\n",
    "\n",
    "# Process each test\n",
    "for test in data:\n",
    "    test_id = test['test_id']\n",
    "    expected = test['expected']\n",
    "    actual = test['actual']\n",
    "    \n",
    "    # Check if this is a summarization test\n",
    "    if 'summarization' in test_id or 'analyze_sentiment_summarize' in test_id:\n",
    "        # Use RAGAS-inspired metrics\n",
    "        accuracy = calculate_ragas_accuracy(expected, actual)\n",
    "    elif isinstance(expected, list) and (not expected or isinstance(expected[0], str)):\n",
    "        # Simple list comparison for string lists\n",
    "        accuracy = calculate_list_accuracy(expected, actual)\n",
    "    else:\n",
    "        # Object comparison for more complex structures\n",
    "        accuracy = calculate_object_accuracy(expected, actual)\n",
    "    \n",
    "    test_accuracies[test_id] = accuracy\n",
    "\n",
    "# Calculate accuracy by test type\n",
    "type_accuracies = {}\n",
    "for test_type, tests in test_groups.items():\n",
    "    type_accuracies[test_type] = sum(test_accuracies[test['test_id']] for test in tests) / len(tests)\n",
    "\n",
    "# Store all results\n",
    "results['test_accuracies'] = test_accuracies\n",
    "results['type_accuracies'] = type_accuracies\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "test_df = pd.DataFrame([{\n",
    "    'test_id': test_id,\n",
    "    'accuracy': accuracy,\n",
    "    'test_type': test_id.split('_')[0]\n",
    "} for test_id, accuracy in test_accuracies.items()])\n",
    "\n",
    "type_df = pd.DataFrame([{\n",
    "    'test_type': test_type,\n",
    "    'accuracy': accuracy,\n",
    "    'test_count': len(test_groups[test_type])\n",
    "} for test_type, accuracy in type_accuracies.items()])\n",
    "\n",
    "# Print results\n",
    "print(\"Overall Accuracy:\", f\"{overall_accuracy:.2%}\")\n",
    "print(\"\\nAccuracy by Test Type:\")\n",
    "print(type_df.sort_values('accuracy', ascending=False))\n",
    "print(\"\\nIndividual Test Accuracies:\")\n",
    "print(test_df.sort_values(['test_type', 'accuracy'], ascending=[True, False]))\n",
    "\n",
    "# Return structured results\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf76d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIRD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
